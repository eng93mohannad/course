# التحيز والقيود [[bias-and-limitations]]


<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
]} />

إذا كنت تنوي استخدام نموذج تم تدريبه مسبقًا أو إصدارًا دقيقًا في الإنتاج ، فيرجى العلم أنه على الرغم من أن هذه النماذج أدوات قوية ، إلا أنها مصحوبة بقيود. أكبرها هو أنه لتمكين التدريب المسبق على كميات كبيرة من البيانات ، غالبًا ما يقوم الباحثون بشق كل المحتوى الذي يمكنهم العثور عليه ، والحصول على أفضل وأسوأ ما هو متاح على الإنترنت.
لإعطاء توضيح سريع ، دعنا نعود إلى مثال خط أنابيب `fill-mask` بنموذج BERT:
```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

عندما يُطلب منك ملء الكلمة المفقودة في هاتين الجملتين ، يقدم النموذج إجابة واحدة فقط خالية من الجنس (نادل / نادلة). أما المهن الأخرى فهي مهن عمل مرتبطة عادةً بجنس واحد محدد - ونعم ، انتهى الأمر بالمومس في أفضل 5 احتمالات يربطها النموذج بـ "المرأة" و "العمل". يحدث هذا على الرغم من أن BERT هو أحد نماذج Transformer النادرة التي لم يتم إنشاؤها عن طريق شق البيانات من جميع أنحاء الإنترنت ، ولكن باستخدام بيانات محايدة ظاهريًا (تم تدريبه على [Wikipedia الإنجليزية](https://huggingface.co/datasets/wikipedia ) و[BookCorpus] (https://huggingface.co/datasets/bookcorpus)).
عند استخدام هذه الأدوات ، يجب أن تضع في اعتبارك أن النموذج الأصلي الذي تستخدمه يمكن أن يولد بسهولة محتوى متحيزًا جنسيًا أو عنصريًا. لن يؤدي ضبط النموذج على بياناتك إلى اختفاء هذا التحيز الجوهري.
